{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters for Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training, validation,\n",
    "# and tesing split.\n",
    "splits = [0.7, 0.9, 1]\n",
    "\n",
    "# Variables to find the average length\n",
    "# of parent and child comments.\n",
    "averageParentLength = 0\n",
    "averageChildLength = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocesser(object):\n",
    "    \"\"\"A class to preprocess text.\"\"\"\n",
    "\n",
    "    def __init__(self, punctuation=True, url=True, number=True):\n",
    "        \"\"\"The init function for the Preprocessor object.\"\"\"\n",
    "\n",
    "        # Initialize the flags for the class\n",
    "        # to tell what tranformations should occur.\n",
    "        self.punctuation = punctuation\n",
    "        self.url = url\n",
    "        self.number = number\n",
    "\n",
    "    def apply(self, text):\n",
    "        \"\"\"A function to apply the desired transformations to the text.\"\"\"\n",
    "        text = self._lowercase(text)\n",
    "\n",
    "        # Check the flags and then execute\n",
    "        # the proper text tranformations.\n",
    "        if self.url:\n",
    "            text = self._remove_url(text)\n",
    "\n",
    "        if self.punctuation:\n",
    "            text = self._remove_punctuation(text)\n",
    "\n",
    "        if self.number:\n",
    "            text = self._remove_number(text)\n",
    "\n",
    "        return re.sub(r'\\s+', ' ', text)  # Return the processed text.\n",
    "\n",
    "    def _remove_punctuation(self, text):\n",
    "        \"\"\"A function to remove all the punctuations in the text.\"\"\"\n",
    "        return re.sub(r'[^\\w\\s]', '', text)  # Remove all punctuation.\n",
    "\n",
    "    def _remove_url(self, text):\n",
    "        \"\"\"A function to remove all the urls in the text.\"\"\"\n",
    "        return re.sub(r'http\\S+', '', text)  # This removes urls from text.\n",
    "\n",
    "    def _remove_number(self, text):\n",
    "        \"\"\"A function to remove all the numbers in the text.\"\"\"\n",
    "        return re.sub(r'[0-9]+', '', text)  # This removes numbers from text.\n",
    "\n",
    "    def _lowercase(self, text):\n",
    "        \"\"\"A function to lowercase the text.\"\"\"\n",
    "        return text.lower()  # This makes text lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1010826 samples in the data set.\n",
      "The average parent comment length is 24.039907956463328\n",
      "The average child comment length is 10.363371143995108\n"
     ]
    }
   ],
   "source": [
    "data_clean = []  # Container to store all clean data.\n",
    "data_unclean = []  # Container to store all unclean data.\n",
    "filename = \"train-balanced-sarcasm.csv\"  # Filename.\n",
    "processer = Preprocesser()  # Instantiate the preprocessor.\n",
    "\n",
    "# Open the file specified by the filename\n",
    "# and parse the data.\n",
    "with open(filename, 'r') as csvfile:\n",
    "    rows = csv.reader(csvfile)  # Get all rows.\n",
    "\n",
    "    # Loop for each row and extract\n",
    "    # necesary data from each row.\n",
    "    for index, row in enumerate(rows):\n",
    "\n",
    "        # Skip the first row as this\n",
    "        # contains column labels.\n",
    "        if index == 0:\n",
    "            continue\n",
    "\n",
    "        # Extract the preprocessed data and append to the data list.\n",
    "        parent = processer.apply(row[9])\n",
    "        child = processer.apply(row[1])\n",
    "\n",
    "        # Code to find the length of the\n",
    "        # average parent and child comments.\n",
    "        averageParentLength += len(parent.split(' '))\n",
    "        averageChildLength += len(child.split(' '))\n",
    "\n",
    "        # Construct the clean data dictionary.\n",
    "        data_clean.append({\"parent\": parent,\n",
    "                     \"child\": child,\n",
    "                     \"label\": [row[0]]})\n",
    "\n",
    "        # Construct the unclean data dictionary.\n",
    "        data_unclean.append({\"parent\": row[9],\n",
    "                \"child\": row[1],\n",
    "                \"label\": [row[0]]})\n",
    "\n",
    "    zipped = list(zip(data_clean, data_unclean))\n",
    "    random.shuffle(zipped)  # Shuffle the data.\n",
    "    data_clean, data_unclean = zip(*zipped)  # Unpack the data.\n",
    "\n",
    "    # Print number of datapoints and\n",
    "    # the average lengths nicely.\n",
    "    print(f\"There are {index} samples in the data set.\")\n",
    "    print(f\"The average parent comment length is {averageParentLength / index}\")\n",
    "    print(f\"The average child comment length is {averageChildLength / index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the data to Training, Validation, and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(data):\n",
    "    \"\"\"\n",
    "    Takes as input data fgrom above and splits it\n",
    "    into training, validation, and testing sets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Lists for the training, validation,\n",
    "    # and testing data.  The length of the\n",
    "    # data and the thresholds for splitting\n",
    "    # the data.\n",
    "    dataSplits = [[] for _ in range(3)]\n",
    "    total = len(data)\n",
    "    thresholds = [total * p for p in splits]\n",
    "\n",
    "    # Loop for all datapoints and place them\n",
    "    # into the correct set, i.e., either\n",
    "    # training, validation, or testing.\n",
    "    for index, datapoint in enumerate(data):\n",
    "        if index < thresholds[0]:\n",
    "            dataSplits[0].append(datapoint)\n",
    "        elif index < thresholds[1]:\n",
    "            dataSplits[1].append(datapoint)\n",
    "        else:\n",
    "            dataSplits[2].append(datapoint)\n",
    "    \n",
    "    # Return the training, validation, and testing sets.\n",
    "    return dataSplits\n",
    "\n",
    "# Split the clean and unclean data.\n",
    "dataSplits_clean = splitData(data_clean)\n",
    "dataSplits_unclean = splitData(data_unclean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the Datasets as jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fileNames_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-b7804a895fcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pad each entry with a '\\n'.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mfileNames_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Save the clean and unclean data splits.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fileNames_' is not defined"
     ]
    }
   ],
   "source": [
    "def saveData(dataSplits, fileNames):\n",
    "    \"\"\"\n",
    "    Takes as input a list of three filesNames, in a list,\n",
    "    fileNames, and a data split, dataSplits, then saves\n",
    "    training, validation, and testing files as JSONL files.\n",
    "    \"\"\"\n",
    "\n",
    "    # Loop for three data sets that need to be created.\n",
    "    for dataSplit, split in zip(dataSplits, fileNames):\n",
    "        # Open a new file to save, named as specified.\n",
    "        with open(f\"./{split}.jsonl\", \"w\") as file:\n",
    "\n",
    "            # Loop for each entry to be saved in the JSON file.\n",
    "            for datapoint in dataSplit:\n",
    "                json.dump(datapoint, file)  # Save each entry.\n",
    "                file.write(\"\\n\")  # pad each entry with a '\\n'.\n",
    "\n",
    "# Define the clean and unclean fileNames.\n",
    "fileNames_clean = [\"training_clean\", \"validation_clean\", \"testing_clean\"]\n",
    "fileNames_unclean = [\"training_unclean\", \"validation_unclean\", \"testing_unclean\"]\n",
    "\n",
    "# Save the clean and unclean data splits.\n",
    "saveData(dataSplits_clean, fileNames_clean)\n",
    "saveData(dataSplits_unclean, fileNames_unclean)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8bbcefd7165799a238ce1f8b683c728a54bc734f4e2e10b5df74ecee63f3034e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
