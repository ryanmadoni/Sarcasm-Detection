{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEb0YR2vLSmN"
      },
      "source": [
        "#### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P37k01edLU-J",
        "outputId": "e2e675b8-1f70-4482-e366-2a8b5a23f612"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSCST0X8LSmT",
        "outputId": "621d16bd-a46d-4b6f-9c06-fb4e2f98db93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.13.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "!pip install transformers\n",
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx8PXvFmLSmV"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Juu3uoB0LSmW"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from transformers import (BertTokenizerFast,\n",
        "                          BertForSequenceClassification,\n",
        "                          DistilBertTokenizerFast,\n",
        "                          DistilBertForSequenceClassification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY4PNwduLSmW"
      },
      "source": [
        "#### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "KOGzCHfKLSmX"
      },
      "outputs": [],
      "source": [
        "# Lists for different parameters for\n",
        "# the different models to be used in\n",
        "# the ensemble methods.  These lists\n",
        "# are all parallel: \"modelNames\", \"fileNames\",\n",
        "# \"modelTypes\", \"taskTypes\",\n",
        "# \"tokenizerTypes\", \"useParentComments\",\n",
        "# \"clean\", and \"maxLengths\".\n",
        "\n",
        "modelNames = [\"Bert Unclean Parent 64\",\n",
        "              \"Bert Unclean Parent 96\",\n",
        "              \"Bert Unclean Parent 128\",\n",
        "              \"DistilBert clean Child 64\",\n",
        "              \"DistilBert clean Child 96\",\n",
        "              \"DistilBert clean Parent 64\",\n",
        "              \"DistilBert clean Parent 96\",\n",
        "              \"DistilBert Unclean Child 64\",\n",
        "              \"DistilBert Unclean Child 96\",\n",
        "              \"DistilBert Unclean Parent 20\",\n",
        "              \"DistilBert Unclean Parent 32\",\n",
        "              \"DistilBert Unclean Parent 64\",\n",
        "              \"DistilBert Unclean Parent 64 Cased\",\n",
        "              \"DistilBert Unclean Parent 96\",\n",
        "              \"DistilBert Unclean Parent 128\"]\n",
        "\n",
        "fileNames = [f\"/content/drive/MyDrive/models/{modelName}/sarcasm\" for modelName in modelNames]\n",
        "\n",
        "modelTypes = [\"bert-base-uncased\",\n",
        "              \"bert-base-uncased\",\n",
        "              \"bert-base-uncased\",\n",
        "              \"distilbert-base-uncased\",\n",
        "              \"distilbert-base-uncased\",\n",
        "              \"distilbert-base-uncased\",\n",
        "              \"distilbert-base-uncased\",\n",
        "              \"distilbert-base-uncased\",\n",
        "              \"distilbert-base-uncased\",\n",
        "              \"distilbert-base-uncased\",\n",
        "              \"distilbert-base-uncased\",\n",
        "              \"distilbert-base-uncased\",\n",
        "              \"distilbert-base-cased\",\n",
        "              \"distilbert-base-uncased\",\n",
        "              \"distilbert-base-uncased\"]\n",
        "\n",
        "taskTypes = [BertForSequenceClassification,\n",
        "             BertForSequenceClassification,\n",
        "             BertForSequenceClassification,\n",
        "             DistilBertForSequenceClassification,\n",
        "             DistilBertForSequenceClassification,\n",
        "             DistilBertForSequenceClassification,\n",
        "             DistilBertForSequenceClassification,\n",
        "             DistilBertForSequenceClassification,\n",
        "             DistilBertForSequenceClassification,\n",
        "             DistilBertForSequenceClassification,\n",
        "             DistilBertForSequenceClassification,\n",
        "             DistilBertForSequenceClassification,\n",
        "             DistilBertForSequenceClassification,\n",
        "             DistilBertForSequenceClassification,\n",
        "             DistilBertForSequenceClassification]\n",
        "\n",
        "tokenizerTypes = [BertTokenizerFast,\n",
        "                  BertTokenizerFast,\n",
        "                  BertTokenizerFast,\n",
        "                  DistilBertTokenizerFast,\n",
        "                  DistilBertTokenizerFast,\n",
        "                  DistilBertTokenizerFast,\n",
        "                  DistilBertTokenizerFast,\n",
        "                  DistilBertTokenizerFast,\n",
        "                  DistilBertTokenizerFast,\n",
        "                  DistilBertTokenizerFast,\n",
        "                  DistilBertTokenizerFast,\n",
        "                  DistilBertTokenizerFast,\n",
        "                  DistilBertTokenizerFast,\n",
        "                  DistilBertTokenizerFast,\n",
        "                  DistilBertTokenizerFast]\n",
        "\n",
        "useParentComments = [True, True, True, False, False, True, True,\n",
        "                     False, False, True, True, True, True, True, True]\n",
        "clean = [False, False, False, True, True, True, True, False,\n",
        "         False, False, False, False, False,  False, False]\n",
        "maxLengths = [64, 96, 128, 64, 96, 64, 96, 64, 96, 20, 32, 64, 64, 96, 128]\n",
        "\n",
        "targets = ['0', '1']  # '0' for not sarcastic.  '1' for sarcastic.\n",
        "\n",
        "# Location for the clean and unclean testing sets.\n",
        "testFileName_unclean = \"/content/drive/MyDrive/data/testing_unclean.jsonl\"\n",
        "testFileName_clean = \"/content/drive/MyDrive/data/testing_clean.jsonl\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQz-CtmKLSmY"
      },
      "source": [
        "#### Load the Testing Data for Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "bXqcGZ_TLSmZ"
      },
      "outputs": [],
      "source": [
        "def parseDataset(fileName):\n",
        "    \"\"\"\n",
        "    Takes as input a fileName of a json file, then opens the\n",
        "    file and returns three lists for the parent and child comments\n",
        "    and labels for sarcastic or not sarcastic.\n",
        "    \"\"\"\n",
        "    parentText, childText, labels = [], [], []  # Instantiate containers.\n",
        "\n",
        "    # Open the training data and convert it to a json list.\n",
        "    with open(fileName, 'r') as json_file:\n",
        "        jsonl = list(json_file)\n",
        "\n",
        "    # Loop through all elements in the json list.\n",
        "    for dataEntry in jsonl:\n",
        "        data = json.loads(dataEntry)  # Load the dictionary.\n",
        "\n",
        "        # Construct the parent, child, and label\n",
        "        # lists that will be returned.\n",
        "        parentText.append(data[\"parent\"])\n",
        "        childText.append(data[\"child\"])\n",
        "        labels.append(int(data[\"label\"][0]))\n",
        "\n",
        "    # Return the data with the parent comment.\n",
        "    return labels, parentText, childText\n",
        "\n",
        "\n",
        "# Initialize the clean and unclean testing sets.\n",
        "testData_clean = parseDataset(testFileName_clean)\n",
        "testData_unclean = parseDataset(testFileName_unclean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brcRuAieLSmZ"
      },
      "source": [
        "#### Function to Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "E2ktadGGLSmb"
      },
      "outputs": [],
      "source": [
        "def predict(*args, tokenizer, maxLength):\n",
        "    \"\"\"\n",
        "    Takes as input a string, text, then predicts if\n",
        "    text is sarcastic (1) or not sarcastic (0).\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the text, then run the input\n",
        "    # through the model and take the argmax\n",
        "    # to get a probability.\n",
        "    inputs = tokenizer(*args, padding=True, truncation=True,\n",
        "                       max_length=maxLength, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model(**inputs)\n",
        "    probs = outputs[0].softmax(1)\n",
        "\n",
        "    # Return whether the text is sarcastic (1)\n",
        "    # or not sarcastic (0).\n",
        "    return probs.argmax()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edy3iIomLSmc"
      },
      "source": [
        "#### Load the models and Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6c0sSNMLSmc",
        "outputId": "6e213bc5-f918-4f3b-b51b-389156ccb02a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model 1/15, Bert Unclean Parent 64 processing\n",
            "model 2/15, Bert Unclean Parent 96 processing\n",
            "model 3/15, Bert Unclean Parent 128 processing\n",
            "model 4/15, DistilBert clean Child 64 processing\n",
            "model 5/15, DistilBert clean Child 96 processing\n",
            "model 6/15, DistilBert clean Parent 64 processing\n",
            "model 7/15, DistilBert clean Parent 96 processing\n",
            "model 8/15, DistilBert Unclean Child 64 processing\n",
            "model 9/15, DistilBert Unclean Child 96 processing\n",
            "model 10/15, DistilBert Unclean Parent 20 processing\n",
            "model 11/15, DistilBert Unclean Parent 32 processing\n",
            "model 12/15, DistilBert Unclean Parent 64 processing\n",
            "model 13/15, DistilBert Unclean Parent 64 Cased processing\n",
            "model 14/15, DistilBert Unclean Parent 96 processing\n",
            "model 15/15, DistilBert Unclean Parent 128 processing\n"
          ]
        }
      ],
      "source": [
        "# Initialize a container to store the predictions\n",
        "# for each model for each testing sample.\n",
        "predictions = np.zeros((len(testData_clean[0]), len(modelTypes)))\n",
        "\n",
        "# Zip the files, the models, the type of the\n",
        "# tasks, and the tokenizer types to loop on.\n",
        "# The enumerate the zipped variables.\n",
        "zipped = enumerate(zip(fileNames, modelTypes, taskTypes, tokenizerTypes, maxLengths))\n",
        "\n",
        "# Loop for the every model and load each model.\n",
        "for modelIndex, (fileName, modelType, taskType, tokenizerType, maxLength) in zipped:\n",
        "    print(f\"model {modelIndex+1}/{len(modelTypes)}, {modelNames[modelIndex]} processing\")\n",
        "\n",
        "    # Load the saved model and tokenizer.\n",
        "    model = taskType.from_pretrained(fileName,\n",
        "                                     num_labels=len(targets)).to(\"cuda\")\n",
        "    tokenizer = tokenizerType.from_pretrained(fileName)\n",
        "\n",
        "    # Loop for all the data in the\n",
        "    # test set and compute the accuracy\n",
        "    # for each sample for this model.\n",
        "    zipped2 = zip(testData_clean[0], *testData_clean[1:], *testData_unclean[1:])\n",
        "    for dataIndex, value in enumerate(zipped2):\n",
        "\n",
        "        # Check if parent comments are enabled for this model\n",
        "        # and if clean or unlcean data is being used.\n",
        "        if useParentComments[modelIndex] and clean[modelIndex]:\n",
        "            text = value[1:3]  # Set text to the clean parent and child comment.\n",
        "        elif not useParentComments[modelIndex] and clean[modelIndex]:\n",
        "            text = [value[2]]  # Set the text to the clean child comment.\n",
        "        elif useParentComments[modelIndex] and not clean[modelIndex]:\n",
        "            text = value[3:5]  # Set text to the unclean parent and child comment.\n",
        "        else:\n",
        "            text = [value[4]]  # Set the text to the unclean child comment.\n",
        "        \n",
        "        # Set the value in the predictions matrix.\n",
        "        prediction = predict(*text, tokenizer=tokenizer, maxLength=maxLength)\n",
        "        predictions[dataIndex, modelIndex] = prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save The Prediction Matrix"
      ],
      "metadata": {
        "id": "71jRWnT-RtuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the prediction matrix as a npy file.\n",
        "# with open(\"/content/drive/MyDrive/ensemble/predictions.npy\", 'wb') as file:\n",
        "#     np.save(file, predictions)"
      ],
      "metadata": {
        "id": "O2Og6WSHRtNL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load The Prediction Matrix"
      ],
      "metadata": {
        "id": "_JCU0NY1SqKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the prediciton matrix.\n",
        "with open(\"/content/drive/MyDrive/ensemble/predictions.npy\", 'rb') as file:\n",
        "    predictions = np.load(file)"
      ],
      "metadata": {
        "id": "Ic9nDJ_aStY6"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Confusion Matrix"
      ],
      "metadata": {
        "id": "rMlIqGLtu_lS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def printConfusionMatrix(true, pred):\n",
        "    \"\"\"\n",
        "    A function to calculate the confusion matrix\n",
        "    and print precisions and recalls.\n",
        "    \"\"\"\n",
        "    (tn, fp), (fn, tp) = confusion_matrix(true, pred)\n",
        "\n",
        "    # Pring precissions and recalls.\n",
        "    print(f\"Positive Precision: {tp/(tp+fp)}\")\n",
        "    print(f\"Negative Precision: {tn/(tn+fn)}\")\n",
        "    print(f\"Positive Recall: {tp/(tp+fn)}\")  \n",
        "    print(f\"Negative Recall: {tn/(tn+fp)}\")"
      ],
      "metadata": {
        "id": "D4-Q9reau-3l"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculate Accuracy"
      ],
      "metadata": {
        "id": "_jMdnk4ZXZHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply a weighting scheme.  This can be changed.  \"weights\"\n",
        "# is a NumPy array that is the length of the number of models where\n",
        "# the probabilities in the list sum must sum to 1.\n",
        "f = 1/14\n",
        "v = 0.001\n",
        "weights = np.array([f-v,f-v,f+13*v,f-v,f-v,f-v,f-v,f-v,f-v,f-v,f-v,0,f-v,f-v,f-v])\n",
        "\n",
        "# A container to store the accuracy values of the models.\n",
        "accuracies = [0]*len(modelTypes)\n",
        "\n",
        "# Calculate the accuracy of each model on its own.\n",
        "for modelIndex in range(len(modelTypes)):\n",
        "    for votes, label in zip(predictions, testData_clean[0]):\n",
        "        accuracies[modelIndex] += (votes[modelIndex] == label)\n",
        "\n",
        "# Loop for all the models and print the metrics for each model.\n",
        "# for index, accuracy in enumerate(accuracies):\n",
        "#     print({modelNames[index]})\n",
        "#     print(f\"The accuracy is {accuracy} / \"\n",
        "#           f\"{len(testData_clean[0])} = {accuracy / len(testData_clean[0])}\")\n",
        "#     printConfusionMatrix(testData_clean[0], predictions[:, index].T)\n",
        "#     print()\n",
        "\n",
        "accuracy = 0  # A value to hold the number of correctly predicted comments.\n",
        "\n",
        "modelPredictions = np.zeros((len(testData_clean[0]), ))  # Predictions for ensemble.\n",
        "accuracy = 0  # Container for the accuracy.\n",
        "\n",
        "# Loop for all the votes, apply the weighting scheme\n",
        "# and check if the prediction is correct.\n",
        "for index, (vote, label) in enumerate(zip(predictions, testData_clean[0])):\n",
        "    modelPredictions[index] = (np.dot(weights, vote) > 0.5)\n",
        "    accuracy += ((np.dot(weights, vote) > 0.5) == label)\n",
        "\n",
        "# Print the accuracy of the ensemble model.\n",
        "print(f\"Ensembling these {len(modelTypes)} models yield an \"\n",
        "      f\"accuracy of {accuracy} / {len(testData_clean[0])} = \"\n",
        "      f\"{accuracy / len(testData_clean[0])}\")\n",
        "printConfusionMatrix(testData_clean[0], modelPredictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOEDcKqRXZao",
        "outputId": "8bf0af1f-48ea-44a2-8734-7063ce216a24"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensembling these 15 models yield an accuracy of 80817 / 101082 = 0.7995192022318514\n",
            "Positive Precision: 0.8119009010858187\n",
            "Negative Precision: 0.7879991597120104\n",
            "Positive Recall: 0.7808551800379027\n",
            "Negative Recall: 0.8182683536270972\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "8bbcefd7165799a238ce1f8b683c728a54bc734f4e2e10b5df74ecee63f3034e"
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}