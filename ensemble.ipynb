{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from transformers import (BertTokenizerFast,\n",
    "                          BertForSequenceClassification,\n",
    "                          DistilBertTokenizerFast,\n",
    "                          DistilBertForSequenceClassification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list for the different fileNames where the weights\n",
    "# are stored for the different models to be used in\n",
    "# the ensemble methods.  This list is parallel to\n",
    "# \"modelTypes\", \"taskTypes\", \"tokenizerTypes\",\n",
    "# and \"useParentComments\".\n",
    "fileNames = [\"file2\",\n",
    "              \"file1\"]\n",
    "\n",
    "# A list for the different models to be used in\n",
    "# the ensemble methods.  This list is parallel to\n",
    "# \"fileNames\", \"taskTypes\", \"tokenizerTypes\",\n",
    "# and \"useParentComments\".\n",
    "modelTypes = [\"bert-base-uncased\",\n",
    "              \"distilbert-base-uncased\"]\n",
    "\n",
    "# A list for the different tasks to be used in\n",
    "# the ensemble methods.  This list is parallel to\n",
    "# \"fileNames\", \"modelTypes\", \"tokenizerTypes\",\n",
    "# and \"useParentComments\".\n",
    "taskTypes = [BertForSequenceClassification,\n",
    "             DistilBertForSequenceClassification]\n",
    "\n",
    "# A list for the different tokenizers to be used in\n",
    "# the ensemble methods.  This list is parallel to\n",
    "# \"fileNames\", \"modelTypes\", \"taskTypes\",\n",
    "# and \"useParentComments\".\n",
    "tokenizerTypes = [BertTokenizerFast,\n",
    "                  DistilBertTokenizerFast]\n",
    "\n",
    "# A list for the different tokenizers to be used in\n",
    "# the ensemble methods.  This list is parallel to\n",
    "# \"fileNames\", \"modelTypes\" and \"taskTypes\".\n",
    "useParentComments = [True, True]\n",
    "\n",
    "targets = ['0', '1']  # '0' for not sarcastic.  '1' for sarcastic.\n",
    "maxLength = 128  # Max length for each comment.\n",
    "testFileName = \"/content/data/testing.json\"  # Location for testing set.\n",
    "\n",
    "# Apply an equal weighting scheme.  This can be changed.  \"weights\"\n",
    "# is a NumPy array that is the length of the number of models where\n",
    "# the probabilities in the list sum must sum to 1.\n",
    "weights = np.array([1 / len(modelTypes) for _ in len(modelTypes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Testing Data for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseDataset(fileName):\n",
    "    \"\"\"\n",
    "    Takes as input a fileName of a json file, then opens the\n",
    "    file and returns three lists for the parent and child comments\n",
    "    and labels for sarcastic or not sarcastic.\n",
    "    \"\"\"\n",
    "    parentText, childText, labels = [], [], []  # Instantiate containers.\n",
    "\n",
    "    # Open the training data and convert it to a json list.\n",
    "    with open(fileName, 'r') as json_file:\n",
    "        jsonl = list(json_file)\n",
    "\n",
    "    # Loop through all elements in the json list.\n",
    "    for dataEntry in jsonl:\n",
    "        data = json.loads(dataEntry)  # Load the dictionary.\n",
    "\n",
    "        # Construct the parent, child, and label\n",
    "        # lists that will be returned.\n",
    "        parentText.append(data[\"parent\"])\n",
    "        childText.append(data[\"child\"])\n",
    "        labels.append(int(data[\"label\"][0]))\n",
    "\n",
    "    # Return the data with the parent comment.\n",
    "    return labels, parentText, childText\n",
    "\n",
    "\n",
    "# Initialize the testing set.\n",
    "testData = parseDataset(testFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(*args, tokenizer):\n",
    "    \"\"\"\n",
    "    Takes as input a string, text, then predicts if\n",
    "    text is sarcastic (1) or not sarcastic (0).\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the text, then run the input\n",
    "    # through the model and take the argmax\n",
    "    # to get a probability.\n",
    "    inputs = tokenizer(*args, padding=True, truncation=True,\n",
    "                       max_length=maxLength, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs[0].softmax(1)\n",
    "\n",
    "    # Return whether the text is sarcastic (1)\n",
    "    # or not sarcastic (0).\n",
    "    return probs.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a container to store the predictions\n",
    "# for each model for each testing sample.\n",
    "predictions = np.zeros((len(testData), len(modelTypes)))\n",
    "\n",
    "# Zip the files, the models, the type of the\n",
    "# tasks, and the tokenizer types to loop on.\n",
    "# The enumerate the zipped variables.\n",
    "zipped = enumerate(zip(fileNames, modelTypes, taskTypes, tokenizerTypes))\n",
    "\n",
    "# Loop for the every model and load each model.\n",
    "for modelIndex, fileName, modelType, taskType, tokenizerType in zipped:\n",
    "\n",
    "    # Load the saved model and tokenizer.\n",
    "    model = taskType.from_pretrained(fileName,\n",
    "                                     num_labels=len(targets)).to(\"cuda\")\n",
    "    tokenizer = tokenizerType.from_pretrained(fileName)\n",
    "\n",
    "    # Loop for all the data in the\n",
    "    # test set and compute the accuracy\n",
    "    # for each sample for this model.\n",
    "    for dataIndex, value in enumerate(zip(testData[0], *testData[1:])):\n",
    "\n",
    "        # Check if parent comments are enabled for this model.\n",
    "        if useParentComments[modelIndex]:\n",
    "            text = value[1:]  # Set text to parent and child comment.\n",
    "        else:\n",
    "            text = [value[2]]  # Set the text to the child comment.\n",
    "        \n",
    "        # Set the value in the predictions matrix.\n",
    "        predictions[dataIndex, modelIndex] = (predict(*text) == value[0])\n",
    "\n",
    "# Loop for all the models and calculate the accuracy of each model.\n",
    "for index, accuracy in enumerate(np.sum(predictions, axis=0)):\n",
    "    print(f\"The accuracy of model {index} is {accuracy} / \"\n",
    "          f\"{len(testData)} = {accuracy / len(testData)}\")\n",
    "\n",
    "accuracy = 0  # A value to hold the number of correctly predicted comments.\n",
    "\n",
    "# Loop for all the votes, apply the weighting scheme\n",
    "# and check if the prediction is correct\n",
    "for vote, label in zip(predictions, testData[0]):\n",
    "    accuracy += ((np.dot(weights, vote) > 0.5) == label)\n",
    "\n",
    "# Print the accuracy of the ensemble model.\n",
    "print(f\"Ensembling these {len(modelTypes)} models yield an \"\n",
    "      f\"accuracy of {accuracy} / {len(testData)} = \"\n",
    "      f\"{accuracy / len(testData)}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8bbcefd7165799a238ce1f8b683c728a54bc734f4e2e10b5df74ecee63f3034e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
